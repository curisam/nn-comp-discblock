# Block-wise Word Embedding Compression Revisited: Better Weighting and Structuring

### Overview
This repository contains the official implementation for the papaer, "Block-wise Word Embedding Compression Revisited: Better Weighting and Structuring", Findings of EMNLP 2021.

### Requirements
* python3, torch, scipy, numpy for discblock. For each task, you need to install required packages.

### Getting Started
This repository provides various tasks for experiments.
Let us introduce how to apply our method with language modeling.

#### Step 1: Train a base model.

#### Step 2: Compute the TF-IDF based Word Importance Score

#### Step 3: Compute Differentiable Word importance Score

#### Step 4: Block-wise Embedding Compression and Fine-tuning

### Tasks

#### Language Modeling

#### Neural Machine Translation

#### SNLI

#### SST-5

#### Knowledge Embedding

### Acknowledgement
This work was supported by Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government(MSIT) (No. 2021-0-00907, Development of Adaptive and Lightweight Edge-Collaborative Analysis Technology for Enabling Proactively Immediate Response and Rapid Learning).
